{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explain the definition of Gini impurity and entropy in decision tree; and explore their similarity and connection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini impurity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a data with $d$ classes $\\{C_1, \\dots, C_d\\}$ with each class prob $p_i$, the Gini impurity is \n",
    "$$ \\mbox{Gini} = \\sum_{i=1}^d (1-p_i) p_i  = 1 - \\sum_{i=1}^d p_i^2. \n",
    "$$\n",
    "Note that $p_i * (1-p_i) = \\mbox{(weight of $C_i$) * (prob of misclassifying $C_i$ as $C_j, j\\ne i$)}. $\n",
    "\n",
    "\n",
    "\n",
    "It measures the uncertainty in the dataset, bounded below with $0$ (certain). Gini Impurity is a measurement of the likelihood of an incorrect classification of a new instance of a random variable, if that new instance were randomly classified according to the distribution of class labels from the data set. For example, \n",
    "- if there are three classes with equal probability (ratio) in a dataset, the Gini impurity is $2/3$. This means if a random class is selected for an instance, it will misclassified with prob $=2/3$.\n",
    "- if there are three classes with ratios $(298, 1,1)/300$, the prob of misclassifying an instance with randomly selected class is $$1 - \\frac{(298^2 +1 +1)}{300^2}$$\n",
    "- For a pupulation with $d$ classes, we compare Gini index of samples of same size: the smaller the Gini index, the less likely to misclassify an instance on average. \n",
    "\n",
    "Suppose we have compute the Gini indices for all features on a dataset, which feature should we choose to split? We want to choose a feature with mimimal Gini impurity; in other words, we want to maximize the **Gini gain**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In information theory, the (Shannon) entropy for a random variable is a measure of level of \"information\", \"surprise\"  or \n",
    "\"uncertainty\" in the possible outcomes. Given a discrete variable $X$, with possible outcomes $x_1,\\dots,x_n$ and prob \n",
    "$p_1, \\dots,p_n$, the entropy of $X$ is defined as \n",
    "$$ H(X) = - \\sum_{i=1}^n p_i \\log p_i\n",
    "$$\n",
    "Intutively, when all outcomes are equally probable, $H(X)$ is maximal. </p>\n",
    "\n",
    "Any analogy or connection between Gini impurity and entropy? \n",
    "- Note that both $1-p_i$ and $-\\log p_i = \\log p_i^{-1}$ are decreasing function of $p_i$\n",
    "- More precise connection is given by Maclaurin formula  $$\n",
    "\\ln(1+x) = x -\\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} + \\cdots\n",
    "$$\n",
    "Now we see $$ -\\ln p = -\\ln (1 + p - 1) = \n",
    "\\left[(1- p) -\\frac{((1-p)^2}{2} + \\frac{(1-p)^3}{3} - \\frac{(1-p)^4}{4} + \\cdots\n",
    "\\right].\n",
    "$$\n",
    "The domimate term of $-p_i \\log p_i$ is $p_i(1-p_i)$, here the connection of Gini and entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To to next:** I will add more examples later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
